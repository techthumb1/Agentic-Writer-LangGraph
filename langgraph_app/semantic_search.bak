# Location: langgraph_app/semantic_search.py
"""
Semantic Search and Content Intelligence System
Provides vector embeddings, similarity search, and intelligent content recommendations
"""

import os
import json
import asyncio
import logging
import numpy as np
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime
import hashlib
import pickle
from abc import ABC, abstractmethod

try:
    import openai
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import redis.asyncio as redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

try:
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.warning("scikit-learn not available. Install with: pip install scikit-learn")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ContentEmbedding:
    """Content with vector embedding"""
    content_id: str
    content_text: str
    content_type: str
    template_id: str
    style_profile: str
    embedding: np.ndarray
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
    final_state = None  # inserted for safety
        if final_state:
            return {**final_state,
        else:
            return {"error": "final_state undefined", "status": "failed"}
            "content_id": self.content_id,
            "content_text": self.content_text,
            "content_type": self.content_type,
            "template_id": self.template_id,
            "style_profile": self.style_profile,
            "embedding": self.embedding.tolist(),
            "metadata": self.metadata,
            "created_at": self.created_at.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ContentEmbedding':
        """Create from dictionary"""
        return cls(
            content_id=data["content_id"],
            content_text=data["content_text"],
            content_type=data["content_type"],
            template_id=data["template_id"],
            style_profile=data["style_profile"],
            embedding=np.array(data["embedding"]),
            metadata=data.get("metadata", {}),
            created_at=datetime.fromisoformat(data["created_at"])
        )

@dataclass
class SearchResult:
    """Search result with similarity score"""
    content_embedding: ContentEmbedding
    similarity_score: float
    rank: int
    explanation: Optional[str] = None

class BaseEmbeddingProvider(ABC):
    """Abstract base for embedding providers"""
    
    @abstractmethod
    async def get_embedding(self, text: str) -> np.ndarray:
        """Generate embedding for text"""
        pass
    
    @abstractmethod
    def get_embedding_dimension(self) -> int:
        """Get embedding vector dimension"""
        pass

class OpenAIEmbeddingProvider(BaseEmbeddingProvider):
    """OpenAI embeddings provider"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "text-embedding-3-small"):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model
        self.client = None
        self.dimension = 1536 if "small" in model else 3072
        
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI package not available")
    
    async def initialize(self) -> bool:
        """Initialize OpenAI client"""
        try:
            self.client = AsyncOpenAI(api_key=self.api_key)
            # Test with a simple embedding
            await self.get_embedding("test")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI embeddings: {e}")
            return False
    
    async def get_embedding(self, text: str) -> np.ndarray:
        """Generate embedding using OpenAI"""
        if not self.client:
            raise Exception("OpenAI client not initialized")
        
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text.replace("\n", " ")[:8000]  # Limit input length
            )
            
            embedding = np.array(response.data[0].embedding)
            return embedding
            
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            raise
    
    def get_embedding_dimension(self) -> int:
        """Get embedding dimension"""
        return self.dimension

class LocalEmbeddingProvider(BaseEmbeddingProvider):
    """Simple local embedding provider using TF-IDF (fallback)"""
    
    def __init__(self):
        self.dimension = 300  # Fixed dimension for simplicity
        self.vocabulary: Dict[str, int] = {}
        self.idf_scores: Dict[str, float] = {}
        
    async def get_embedding(self, text: str) -> np.ndarray:
        """Generate simple TF-IDF based embedding"""
        # This is a very simplified embedding - in production use proper models
        words = text.lower().split()
        
        # Create simple bag of words with position weighting
        embedding = np.zeros(self.dimension)
        
        for i, word in enumerate(words[:self.dimension]):
            # Simple hash-based embedding
            hash_val = abs(hash(word)) % self.dimension
            embedding[hash_val] += 1.0 / (i + 1)  # Position weighting
        
        # Normalize
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def get_embedding_dimension(self) -> int:
        return self.dimension

class VectorStore:
    """Vector storage and similarity search"""
    
    def __init__(self, redis_url: Optional[str] = None, 
                 namespace: str = "content_embeddings"):
        self.redis_url = redis_url
        self.namespace = namespace
        self.redis_client: Optional[redis.Redis] = None
        self.local_store: Dict[str, ContentEmbedding] = {}
        self.use_redis = redis_url is not None and REDIS_AVAILABLE
    
    async def initialize(self) -> bool:
        """Initialize vector store"""
        if self.use_redis:
            try:
                self.redis_client = redis.from_url(self.redis_url)
                await self.redis_client.ping()
                logger.info("Redis vector store initialized")
                return True
            except Exception as e:
                logger.warning(f"Redis initialization failed, using local store: {e}")
                self.use_redis = False
        
        logger.info("Using local vector store")
        return True
    
    async def store_embedding(self, embedding: ContentEmbedding) -> bool:
        """Store content embedding"""
        try:
            if self.use_redis and self.redis_client:
                # Store in Redis
                key = f"{self.namespace}:{embedding.content_id}"
                data = json.dumps(embedding.to_dict())
                await self.redis_client.set(key, data)
                
                # Also store in search index
                await self._add_to_search_index(embedding)
            else:
                # Store locally
                self.local_store[embedding.content_id] = embedding
            
            logger.info(f"Stored embedding for content {embedding.content_id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to store embedding: {e}")
            return False
    
    async def get_embedding(self, content_id: str) -> Optional[ContentEmbedding]:
        """Retrieve content embedding by ID"""
        try:
            if self.use_redis and self.redis_client:
                key = f"{self.namespace}:{content_id}"
                data = await self.redis_client.get(key)
                if data:
                    return ContentEmbedding.from_dict(json.loads(data.decode('utf-8')))
            else:
                return self.local_store.get(content_id)
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to get embedding: {e}")
            return None
    
    async def search_similar(self, query_embedding: np.ndarray, 
                           limit: int = 10, 
                           filters: Optional[Dict[str, Any]] = None) -> List[SearchResult]:
        """Search for similar content"""
        try:
            if self.use_redis and self.redis_client:
                return await self._redis_similarity_search(query_embedding, limit, filters)
            else:
                return await self._local_similarity_search(query_embedding, limit, filters)
                
        except Exception as e:
            logger.error(f"Similarity search failed: {e}")
            return []
    
    async def _local_similarity_search(self, query_embedding: np.ndarray, 
                                     limit: int, 
                                     filters: Optional[Dict[str, Any]]) -> List[SearchResult]:
        """Local similarity search using cosine similarity"""
        if not SKLEARN_AVAILABLE:
            logger.error("scikit-learn required for local similarity search")
            return []
        
        candidates = []
        
        for content_id, embedding in self.local_store.items():
            # Apply filters
            if filters and not self._apply_filters(embedding, filters):
                continue
            
            # Calculate similarity
            similarity = cosine_similarity(
                query_embedding.reshape(1, -1),
                embedding.embedding.reshape(1, -1)
            )[0][0]
            
            candidates.append((embedding, similarity))
        
        # Sort by similarity and limit
        candidates.sort(key=lambda x: x[1], reverse=True)
        candidates = candidates[:limit]
        
        # Create search results
        results = []
        for rank, (embedding, similarity) in enumerate(candidates):
            results.append(SearchResult(
                content_embedding=embedding,
                similarity_score=similarity,
                rank=rank + 1
            ))
        
        return results
    
    async def _redis_similarity_search(self, query_embedding: np.ndarray, 
                                     limit: int, 
                                     filters: Optional[Dict[str, Any]]) -> List[SearchResult]:
        """Redis-based similarity search"""
        # For Redis vector search, you'd typically use Redis modules like RediSearch
        # For now, fall back to loading all and doing local search
        logger.warning("Redis vector search not fully implemented, using local fallback")
        
        # Load all embeddings from Redis
        pattern = f"{self.namespace}:*"
        keys = await self.redis_client.keys(pattern)
        
        candidates = []
        for key in keys:
            data = await self.redis_client.get(key)
            if data:
                embedding = ContentEmbedding.from_dict(json.loads(data.decode('utf-8')))
                
                # Apply filters
                if filters and not self._apply_filters(embedding, filters):
                    continue
                
                # Calculate similarity
                if SKLEARN_AVAILABLE:
                    similarity = cosine_similarity(
                        query_embedding.reshape(1, -1),
                        embedding.embedding.reshape(1, -1)
                    )[0][0]
                    
                    candidates.append((embedding, similarity))
        
        # Sort and limit
        candidates.sort(key=lambda x: x[1], reverse=True)
        candidates = candidates[:limit]
        
        # Create results
        results = []
        for rank, (embedding, similarity) in enumerate(candidates):
            results.append(SearchResult(
                content_embedding=embedding,
                similarity_score=similarity,
                rank=rank + 1
            ))
        
        return results
    
    def _apply_filters(self, embedding: ContentEmbedding, 
                      filters: Dict[str, Any]) -> bool:
        """Apply search filters"""
        for key, value in filters.items():
            if key == "content_type" and embedding.content_type != value:
                return False
            elif key == "template_id" and embedding.template_id != value:
                return False
            elif key == "style_profile" and embedding.style_profile != value:
                return False
            elif key in embedding.metadata and embedding.metadata[key] != value:
                return False
        
        return True
    
    async def _add_to_search_index(self, embedding: ContentEmbedding):
        """Add embedding to search index (for future Redis vector search implementation)"""
        # Placeholder for future Redis vector search index
        pass
    
    async def get_stats(self) -> Dict[str, Any]:
        """Get vector store statistics"""
        if self.use_redis and self.redis_client:
            pattern = f"{self.namespace}:*"
            keys = await self.redis_client.keys(pattern)
            total_embeddings = len(keys)
        else:
            total_embeddings = len(self.local_store)
        
        if final_state:
            return {**final_state,
        else:
            return {"error": "final_state undefined", "status": "failed"}
            "total_embeddings": total_embeddings,
            "storage_type": "redis" if self.use_redis else "local",
            "namespace": self.namespace
        }

class ContentIntelligence:
    """Content intelligence and recommendation engine"""
    
    def __init__(self, embedding_provider: BaseEmbeddingProvider, 
                 vector_store: VectorStore):
        self.embedding_provider = embedding_provider
        self.vector_store = vector_store
        self.content_clusters: Dict[str, List[str]] = {}
        self.trend_analysis: Dict[str, Any] = {}
    
    async def index_content(self, content_id: str, content_text: str, 
                          content_type: str, template_id: str, 
                          style_profile: str, metadata: Optional[Dict[str, Any]] = None) -> bool:
        """Index content for semantic search"""
        try:
            # Generate embedding
            embedding_vector = await self.embedding_provider.get_embedding(content_text)
            
            # Create content embedding
            content_embedding = ContentEmbedding(
                content_id=content_id,
                content_text=content_text,
                content_type=content_type,
                template_id=template_id,
                style_profile=style_profile,
                embedding=embedding_vector,
                metadata=metadata or {}
            )
            
            # Store in vector store
            success = await self.vector_store.store_embedding(content_embedding)
            
            if success:
                logger.info(f"Successfully indexed content {content_id}")
                
                # Update clusters asynchronously
                asyncio.create_task(self._update_clusters())
            
            return success
            
        except Exception as e:
            logger.error(f"Failed to index content {content_id}: {e}")
            return False
    
    async def search_content(self, query: str, limit: int = 10, 
                           filters: Optional[Dict[str, Any]] = None) -> List[SearchResult]:
        """Search for similar content using semantic search"""
        try:
            # Generate query embedding
            query_embedding = await self.embedding_provider.get_embedding(query)
            
            # Search similar content
            results = await self.vector_store.search_similar(
                query_embedding, limit, filters
            )
            
            # Add explanations
            for result in results:
                result.explanation = self._generate_search_explanation(
                    query, result.content_embedding, result.similarity_score
                )
            
            return results
            
        except Exception as e:
            logger.error(f"Content search failed: {e}")
            return []
    
    async def recommend_templates(self, content_description: str, 
                                limit: int = 5) -> List[Dict[str, Any]]:
        """Recommend templates based on content description"""
        try:
            # Search for similar content
            results = await self.search_content(content_description, limit * 2)
            
            # Aggregate by template
            template_scores: Dict[str, List[float]] = {}
            template_examples: Dict[str, ContentEmbedding] = {}
            
            for result in results:
                template_id = result.content_embedding.template_id
                if template_id not in template_scores:
                    template_scores[template_id] = []
                    template_examples[template_id] = result.content_embedding
                
                template_scores[template_id].append(result.similarity_score)
            
            # Calculate average scores and rank
            template_recommendations = []
            for template_id, scores in template_scores.items():
                avg_score = sum(scores) / len(scores)
                template_recommendations.append({
                    "template_id": template_id,
                    "relevance_score": avg_score,
                    "example_content": template_examples[template_id].content_text[:200] + "...",
                    "usage_count": len(scores),
                    "style_profiles": list(set([
                        r.content_embedding.style_profile 
                        for r in results 
                        if r.content_embedding.template_id == template_id
                    ]))
                })
            
            # Sort by relevance
            template_recommendations.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            return template_recommendations[:limit]
            
        except Exception as e:
            logger.error(f"Template recommendation failed: {e}")
            return []
    
    async def find_content_gaps(self, topic_area: str) -> List[Dict[str, Any]]:
        """Identify content gaps in a topic area"""
        try:
            # Search existing content
            existing_content = await self.search_content(topic_area, limit=50)
            
            if not existing_content:
                return [{
                    "gap_type": "no_existing_content",
                    "description": f"No existing content found for {topic_area}",
                    "priority": "high",
                    "suggested_templates": []
                }]
            
            # Analyze coverage
            gaps = []
            
            # Check for style diversity
            style_profiles = set(r.content_embedding.style_profile for r in existing_content)
            if len(style_profiles) < 3:
                gaps.append({
                    "gap_type": "style_diversity",
                    "description": f"Limited style diversity for {topic_area}",
                    "priority": "medium",
                    "existing_styles": list(style_profiles),
                    "suggested_styles": ["technical_tutor", "popular_sci", "beginner_friendly"]
                })
            
            # Check for content type diversity
            content_types = set(r.content_embedding.content_type for r in existing_content)
            if len(content_types) < 2:
                gaps.append({
                    "gap_type": "content_type_diversity",
                    "description": f"Limited content type diversity for {topic_area}",
                    "priority": "medium",
                    "existing_types": list(content_types),
                    "suggested_types": ["tutorial", "reference", "example", "comparison"]
                })
            
            return gaps
            
        except Exception as e:
            logger.error(f"Content gap analysis failed: {e}")
            return []
    
    async def analyze_content_clusters(self, num_clusters: int = 5) -> Dict[str, Any]:
        """Analyze content clusters to understand topic distribution"""
        if not SKLEARN_AVAILABLE:
            logger.error("scikit-learn required for cluster analysis")
            if final_state:
                return {**final_state, }
            else:
                return {"error": "final_state undefined", "status": "failed"}
        
        try:
            # Get all embeddings
            all_embeddings = []
            content_info = []
            
            if self.vector_store.use_redis and self.vector_store.redis_client:
                # Load from Redis
                pattern = f"{self.vector_store.namespace}:*"
                keys = await self.vector_store.redis_client.keys(pattern)
                
                for key in keys:
                    data = await self.vector_store.redis_client.get(key)
                    if data:
                        embedding = ContentEmbedding.from_dict(json.loads(data.decode('utf-8')))
                        all_embeddings.append(embedding.embedding)
                        content_info.append({
                            "content_id": embedding.content_id,
                            "template_id": embedding.template_id,
                            "style_profile": embedding.style_profile,
                            "content_type": embedding.content_type
                        })
            else:
                # Load from local store
                for embedding in self.vector_store.local_store.values():
                    all_embeddings.append(embedding.embedding)
                    content_info.append({
                        "content_id": embedding.content_id,
                        "template_id": embedding.template_id,
                        "style_profile": embedding.style_profile,
                        "content_type": embedding.content_type
                    })
            
            if len(all_embeddings) < num_clusters:
                return {**state, "error": "Not enough content for cluster analysis"}
            
            # Perform clustering
            embeddings_matrix = np.array(all_embeddings)
            kmeans = KMeans(n_clusters=num_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(embeddings_matrix)
            
            # Analyze clusters
            clusters = {}
            for i in range(num_clusters):
                cluster_indices = np.where(cluster_labels == i)[0]
                cluster_content = [content_info[idx] for idx in cluster_indices]
                
                # Analyze cluster characteristics
                templates = [c["template_id"] for c in cluster_content]
                styles = [c["style_profile"] for c in cluster_content]
                types = [c["content_type"] for c in cluster_content]
                
                clusters[f"cluster_{i}"] = {
                    "size": len(cluster_content),
                    "most_common_template": max(set(templates), key=templates.count),
                    "most_common_style": max(set(styles), key=styles.count),
                    "most_common_type": max(set(types), key=types.count),
                    "template_distribution": {t: templates.count(t) for t in set(templates)},
                    "style_distribution": {s: styles.count(s) for s in set(styles)},
                    "type_distribution": {t: types.count(t) for t in set(types)},
                    "content_ids": [c["content_id"] for c in cluster_content]
                }
            
            # Store cluster information
            self.content_clusters = {
                cluster_id: cluster_data["content_ids"] 
                for cluster_id, cluster_data in clusters.items()
            }
            
            return {**state, 
                "clusters": clusters,
                "total_content": len(all_embeddings),
                "cluster_count": num_clusters,
                "analysis_timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Cluster analysis failed: {e}")
            return {**state, "error": str(e)}
    
    def _generate_search_explanation(self, query: str, content: ContentEmbedding, 
                                   similarity_score: float) -> str:
        """Generate explanation for search result"""
        if similarity_score > 0.9:
            relevance = "highly relevant"
        elif similarity_score > 0.7:
            relevance = "relevant"
        elif similarity_score > 0.5:
            relevance = "somewhat relevant"
        else:
            relevance = "loosely related"
        
        return f"This {content.content_type} content is {relevance} to your query about '{query}' (similarity: {similarity_score:.2f})"
    
    async def _update_clusters(self):
        """Update content clusters (background task)"""
        try:
            # Run cluster analysis with fewer clusters for frequent updates
            await self.analyze_content_clusters(num_clusters=3)
        except Exception as e:
            logger.warning(f"Background cluster update failed: {e}")

# Factory functions
async def create_semantic_search_system(
    embedding_provider_type: str = "openai",
    redis_url: Optional[str] = None,
    openai_api_key: Optional[str] = None
) -> Tuple[BaseEmbeddingProvider, VectorStore, ContentIntelligence]:
    """Create complete semantic search system"""
    
    # Create embedding provider
    if embedding_provider_type.lower() == "openai" and OPENAI_AVAILABLE:
        embedding_provider = OpenAIEmbeddingProvider(api_key=openai_api_key)
        if not await embedding_provider.initialize():
            logger.warning("OpenAI embeddings failed, falling back to local")
            embedding_provider = LocalEmbeddingProvider()
    else:
        embedding_provider = LocalEmbeddingProvider()
    
    # Create vector store
    vector_store = VectorStore(redis_url=redis_url)
    await vector_store.initialize()
    
    # Create content intelligence
    content_intelligence = ContentIntelligence(embedding_provider, vector_store)
    
    return embedding_provider, vector_store, content_intelligence

# Example usage
async def example_usage():
    """Example of semantic search system usage"""
    print("=== Semantic Search System Example ===")
    
    # Create system
    embedding_provider, vector_store, content_intelligence = await create_semantic_search_system(
        embedding_provider_type="local",  # Use local for demo
        redis_url=None  # Use local storage for demo
    )
    
    # Sample content to index
    sample_content = [
        {
            "content_id": "ml_intro_1",
            "content_text": "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It involves algorithms that improve automatically through experience.",
            "content_type": "tutorial",
            "template_id": "technical_tutorial",
            "style_profile": "educational_expert",
            "metadata": {"difficulty": "beginner", "topics": ["ML", "AI"]}
        },
        {
            "content_id": "ai_ethics_1", 
            "content_text": "AI ethics involves ensuring artificial intelligence systems are developed and used responsibly. Key considerations include fairness, transparency, privacy, and accountability in AI decision-making.",
            "content_type": "article",
            "template_id": "ai_ethics_story",
            "style_profile": "popular_sci",
            "metadata": {"difficulty": "intermediate", "topics": ["AI", "ethics"]}
        },
        {
            "content_id": "startup_ml_1",
            "content_text": "Startups can leverage machine learning to gain competitive advantages through personalized recommendations, predictive analytics, and automated processes. Success requires quality data and clear objectives.",
            "content_type": "guide",
            "template_id": "startup_usecases", 
            "style_profile": "founder_storytelling",
            "metadata": {"difficulty": "intermediate", "topics": ["ML", "startups"]}
        }
    ]
    
    # Index content
    print("Indexing sample content...")
    for content in sample_content:
        success = await content_intelligence.index_content(**content)
        print(f"  Indexed {content['content_id']}: {success}")
    
    # Test semantic search
    print("\n=== Testing Semantic Search ===")
    queries = [
        "artificial intelligence learning",
        "startup machine learning applications", 
        "responsible AI development"
    ]
    
    for query in queries:
        print(f"\nQuery: '{query}'")
        results = await content_intelligence.search_content(query, limit=3)
        
        for result in results:
            print(f"  Rank {result.rank}: {result.content_embedding.content_id}")
            print(f"    Similarity: {result.similarity_score:.3f}")
            print(f"    Type: {result.content_embedding.content_type}")
            print(f"    Template: {result.content_embedding.template_id}")
            if result.explanation:
                print(f"    Explanation: {result.explanation}")
    
    # Test template recommendations
    print("\n=== Testing Template Recommendations ===")
    description = "I want to create content about deep learning for beginners"
    recommendations = await content_intelligence.recommend_templates(description)
    
    print(f"Recommendations for: '{description}'")
    for rec in recommendations:
        print(f"  Template: {rec['template_id']}")
        print(f"    Relevance: {rec['relevance_score']:.3f}")
        print(f"    Styles: {rec['style_profiles']}")
    
    # Test content gap analysis
    print("\n=== Testing Content Gap Analysis ===")
    gaps = await content_intelligence.find_content_gaps("machine learning")
    
    print("Content gaps for 'machine learning':")
    for gap in gaps:
        print(f"  Gap: {gap['gap_type']}")
        print(f"    Priority: {gap['priority']}")
        print(f"    Description: {gap['description']}")
    
    # Test cluster analysis
    print("\n=== Testing Cluster Analysis ===")
    cluster_analysis = await content_intelligence.analyze_content_clusters(num_clusters=2)
    
    if "clusters" in cluster_analysis:
        print("Content clusters:")
        for cluster_id, cluster_data in cluster_analysis["clusters"].items():
            print(f"  {cluster_id}: {cluster_data['size']} items")
            print(f"    Main template: {cluster_data['most_common_template']}")
            print(f"    Main style: {cluster_data['most_common_style']}")
    
    # Get system stats
    print("\n=== System Statistics ===")
    stats = await vector_store.get_stats()
    print(f"Vector store stats: {stats}")

if __name__ == "__main__":
    asyncio.run(example_usage())